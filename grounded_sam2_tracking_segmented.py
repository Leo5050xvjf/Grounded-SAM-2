# ==============================================================
# export_gsam2_to_yolo11seg_egg_robust.py
# ä»¥ Grounding DINO + SAM2 ç”¢ç”Ÿã€Œeggã€åˆ†å‰²ï¼ŒåŒ¯å‡ºç‚º YOLOv11-seg è³‡æ–™é›†
# é‡é»ï¼šæŠ—é®æ“‹ã€é˜²éŒ¯èª¤æŒçºŒï¼ˆä½ä¿¡å¿ƒå³è¦–ç‚ºç„¡ç›®æ¨™ï¼›LOST/FOUNDï¼›é€±æœŸ Re-Detectï¼‰
# å‡å‹»åˆ†å‰²æ¯”ä¾‹ï¼šæ¯ 10 å¹€ â†’ 8 train / 1 val / 1 test
# ==============================================================

import os, cv2, torch, numpy as np
from pathlib import Path
from PIL import Image
from contextlib import nullcontext
import supervision as sv
from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection

# === SAM2 ===
from sam2.build_sam import build_sam2_video_predictor, build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
from utils.video_utils import create_video_from_images

# ======================== ä½¿ç”¨è€…å¯èª¿åƒ =========================
MODEL_ID = "IDEA-Research/grounding-dino-base"   # Grounding DINO
VIDEO_PATH = "./assets/kitchen.mp4"

# å–®ä¸€ç›®æ¨™é¡åˆ¥ï¼šegg
TEXT_PROMPT = "egg white. fried egg."
TARGET_YOLO_NAME = "egg white. fried egg."
GDINO_TEXT = "egg white. fried egg."

SEGMENT_SIZE = 200
INIT_SEARCH_FRAMES = 10
THRESHOLDS = [
    (0.40, 0.30),
    (0.30, 0.22),
    (0.25, 0.18),
    (0.20, 0.15),
]

# ---- æŠ—é®æ“‹&é˜²éŒ¯èª¤æŒçºŒæ ¸å¿ƒåƒæ•¸ ----
REQUIRE_ALWAYS = False      # é—œé–‰å¼·åˆ¶è£œæ´ï¼›ä½ä¿¡å¿ƒå°±ç•¶ä½œã€Œæ­¤å¹€ç„¡ç›®æ¨™ã€
MASK_CONF_MIN = 0.55        # mask å…§éƒ¨å¹³å‡æ©Ÿç‡ä¸‹é™ï¼ˆè¶Šé«˜è¶Šåš´ï¼‰
AREA_FRAC_MIN = 0.001       # é¢ç©å æ¯”ä¸‹é™ï¼ˆä¾è§£æåº¦&é¡é ­è·é›¢èª¿æ•´ï¼‰
AREA_FRAC_MAX = 0.15        # é¢ç©å æ¯”ä¸Šé™ï¼ˆé¿å…åƒåˆ°å¤§å¡ŠèƒŒæ™¯æˆ–æ•´é‹ï¼‰
RECHECK_EVERY = 5           # æ¯éš” N å¹€åšä¸€æ¬¡ DINO é‡æ–°é©—è­‰/é‡å®šä½
LOST_PATIENCE = 3           # é€£çºŒä½ä¿¡å¿ƒå¹€æ•¸é”æ¨™â†’å®£å‘Šéºå¤±ï¼ˆæ¸…æ‰è¿½è¹¤ï¼‰
FOUND_PATIENCE = 2          # é€£çºŒé«˜ä¿¡å¿ƒå¹€æ•¸â†’æ‰åˆ¤å®šæ¢å¾©ï¼ˆæŠ—æŠ–å‹•ï¼‰
IOU_MIN = 0.30              # é‡å®šä½å€™é¸èˆ‡æ­·å²ç©©å®š mask çš„æœ€ä½ IoUï¼ˆç›¸ä¼¼åº¦æª»ï¼‰
DET_CONF_MIN = 0.35         # DINO çš„æœ€ä½æ¥å—åˆ†æ•¸ï¼ˆè‹¥ç‰ˆæœ¬ç„¡ scoresï¼Œç•¶å­˜åœ¨æ——æ¨™ï¼‰

# å¯è¦–åŒ–è¼¸å‡ºï¼ˆé¸é…ï¼‰
OUTPUT_DIR = Path("./seg_tracking_results_egg_robust")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_VIDEO_PATH = "./outputs/kitchen_segmented_tracking_egg_robust.mp4"

# SAM2 æ¬Šé‡èˆ‡è¨­å®š
SAM2_CHECKPOINT = "./checkpoints/sam2.1_hiera_large.pt"
SAM2_CFG = "configs/sam2.1/sam2.1_hiera_l.yaml"

# ===============================================================
device = "cuda" if torch.cuda.is_available() else "cpu"
amp_dtype = torch.bfloat16 if device == "cuda" else torch.float32
if device == "cuda" and torch.cuda.get_device_properties(0).major >= 8:
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

# å»º SAM2 æ¨¡å‹
video_predictor = build_sam2_video_predictor(SAM2_CFG, SAM2_CHECKPOINT)
sam2_image_model = build_sam2(SAM2_CFG, SAM2_CHECKPOINT)
image_predictor = SAM2ImagePredictor(sam2_image_model)

# å»º Grounding DINO
processor = AutoProcessor.from_pretrained(MODEL_ID)
grounding_model = AutoModelForZeroShotObjectDetection.from_pretrained(MODEL_ID).to(device)

# å½±ç‰‡è®€å–
video_info = sv.VideoInfo.from_video_path(VIDEO_PATH)
print(video_info)
frame_gen = sv.get_video_frames_generator(VIDEO_PATH, stride=1, start=1000, end=None)
all_frames = [f for f in frame_gen]
total_frames = len(all_frames)
print(f"Total frames: {total_frames}")

tmp_dir = Path("./tmp_frames")
tmp_dir.mkdir(exist_ok=True)
segment_count = (total_frames + SEGMENT_SIZE - 1) // SEGMENT_SIZE
print(f"Processing in {segment_count} segments...")

# ===============================================================
# Dataset folder setup (8:1:1)
# ===============================================================
DATASET_ROOT = Path("./dataset_egg_robust")
IMG_DIR_TRAIN = DATASET_ROOT / "images/train"
IMG_DIR_VAL   = DATASET_ROOT / "images/val"
IMG_DIR_TEST  = DATASET_ROOT / "images/test"
LBL_DIR_TRAIN = DATASET_ROOT / "labels/train"
LBL_DIR_VAL   = DATASET_ROOT / "labels/val"
LBL_DIR_TEST  = DATASET_ROOT / "labels/test"
for p in [IMG_DIR_TRAIN, IMG_DIR_VAL, IMG_DIR_TEST,
          LBL_DIR_TRAIN, LBL_DIR_VAL, LBL_DIR_TEST]:
    p.mkdir(parents=True, exist_ok=True)

def choose_split(global_idx: int) -> str:
    """æ¯ 10 å¹€ï¼šå‰ 8 å¹€ trainï¼Œç¬¬ 9 å¹€ valï¼Œç¬¬ 10 å¹€ testï¼ˆå‡å‹»åˆ†å¸ƒå…¨ç‰‡ï¼‰"""
    pos_in_block = global_idx % 10
    if pos_in_block < 8:
        return "train"
    elif pos_in_block == 8:
        return "val"
    else:
        return "test"

DATASET_YAML = DATASET_ROOT / "dataset.yaml"

# ===============================================================
# å·¥å…·å‡½å¼
# ===============================================================
def normalize_mask(arr: np.ndarray) -> np.ndarray:
    m = np.asarray(arr)
    m = np.squeeze(m)
    if m.ndim == 3:
        if m.shape[0] in (1,) or (m.shape[0] <= m.shape[-1]):
            m = m[0]
        else:
            m = m[..., 0]
    return (m > 0)

def try_gdino_on_image(image_pil: Image.Image, text: str, box_th: float, text_th: float):
    """å›å‚³ boxes(float32 Nx4 xyxy) èˆ‡ scores(float32 N)ï¼›è‹¥ç„¡ scores å‰‡ç‚º 1."""
    inputs = processor(images=image_pil, text=text, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = grounding_model(**inputs)
    results = processor.post_process_grounded_object_detection(
        outputs, inputs.input_ids,
        box_threshold=box_th, text_threshold=text_th,
        target_sizes=[image_pil.size[::-1]],
    )
    if not results:
        return np.zeros((0, 4), dtype=np.float32), np.zeros((0,), dtype=np.float32)
    r = results[0]
    boxes = r["boxes"].detach().cpu().numpy()
    scores = r.get("scores", None)
    if scores is None:
        scores = np.ones((boxes.shape[0],), dtype=np.float32)
    else:
        scores = scores.detach().cpu().numpy().astype(np.float32)
    return boxes, scores

def find_init_by_searching_first_k_frames(frame_paths, k=10):
    """åœ¨å‰ k å¹€å…§æ‰¾ç¬¬ä¸€å€‹å¯ç”¨åµæ¸¬ï¼›å–åˆ†æ•¸æœ€é«˜çš„ boxã€‚"""
    best = None
    for local_idx in range(min(k, len(frame_paths))):
        pil_img = Image.open(frame_paths[local_idx]).convert("RGB")
        for (bth, tth) in THRESHOLDS:
            boxes, scores = try_gdino_on_image(pil_img, GDINO_TEXT, bth, tth)
            if boxes.shape[0] > 0:
                j = int(np.argmax(scores))
                if scores[j] >= DET_CONF_MIN:
                    best = (local_idx, boxes[j:j+1], float(scores[j]))
                    break
        if best is not None:
            break
    if best is None:
        return None, None
    return best[0], best[1]

def mask_to_normalized_polygon(msk: np.ndarray):
    """mask(bool HxW) -> YOLO normalized polygonï¼ˆåªå–æœ€å¤§å¤–è¼ªå»“ï¼‰"""
    m = (msk.astype(np.uint8) * 255)
    cnts, _ = cv2.findContours(m, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not cnts:
        return []
    cnt = max(cnts, key=cv2.contourArea)
    peri = cv2.arcLength(cnt, True)
    approx = cv2.approxPolyDP(cnt, epsilon=0.002 * peri, closed=True)
    approx = approx.reshape(-1, 2)
    H, W = msk.shape[:2]
    poly = []
    for (x, y) in approx:
        poly.append(x / W)
        poly.append(y / H)
    return poly if len(poly) >= 6 else []

def mask_iou(a: np.ndarray, b: np.ndarray) -> float:
    if a is None or b is None:
        return 0.0
    inter = np.logical_and(a, b).sum()
    union = np.logical_or(a, b).sum()
    return float(inter) / float(union + 1e-9)

def logits_to_conf_and_bin(logits_np: np.ndarray):
    """å¾ logits è¨ˆç®—ï¼šæ©Ÿç‡ã€äºŒå€¼ã€å…§éƒ¨å¹³å‡æ©Ÿç‡ã€é¢ç©å æ¯”"""
    proba = 1.0 / (1.0 + np.exp(-logits_np))
    m_bin = (proba > 0.5)
    p_inside = proba[m_bin].mean() if m_bin.any() else 0.0
    area_frac = float(m_bin.mean())
    return proba, m_bin, p_inside, area_frac

# ===============================================================
# ä¸»æµç¨‹
# ===============================================================
frame_idx_global = 0
prev_segment_mask = None
TARGET_CLASS_ID = 0

# è¿½è¹¤å¥å£¯åŒ–ç‹€æ…‹
lost_cnt = 0
found_cnt = 0
prev_stable_mask = None   # æœ€è¿‘ä¸€æ¬¡å¯ä¿¡çš„ maskï¼ˆåš IoU æ¯”å°ï¼‰

autocast_ctx = torch.autocast(device_type=device, dtype=amp_dtype) if device == "cuda" else nullcontext()
with autocast_ctx:
    for seg_id in range(segment_count):
        start = seg_id * SEGMENT_SIZE
        end = min((seg_id + 1) * SEGMENT_SIZE, total_frames)
        print(f"\n--- Segment {seg_id+1}/{segment_count} | Frames {start}-{end-1} ---")

        # æš«å­˜æ®µ frame
        for f in tmp_dir.glob("*.jpg"):
            f.unlink()
        for i in range(start, end):
            cv2.imwrite(str(tmp_dir / f"{i-start:05d}.jpg"), all_frames[i])
        frame_paths = sorted(tmp_dir.glob("*.jpg"), key=lambda p: int(p.stem))
        inference_state = video_predictor.init_state(video_path=str(tmp_dir))

        # åˆå§‹åŒ–ï¼šè‹¥ä¸Šä¸€æ®µéºå¤±ï¼Œå°±é‡æ–°æœå°‹ï¼›è‹¥ä¸Šä¸€æ®µç©©å®šå°±ç”¨ mask æ¥åŠ›
        if prev_segment_mask is None:
            init_idx, boxes = find_init_by_searching_first_k_frames(frame_paths, k=INIT_SEARCH_FRAMES)
            if init_idx is not None:
                pil_img = Image.open(frame_paths[init_idx]).convert("RGB")
                image_predictor.set_image(np.array(pil_img))
                masks_img, _, _ = image_predictor.predict(
                    point_coords=None, point_labels=None,
                    box=boxes, multimask_output=False
                )
                if masks_img is not None and masks_img.ndim == 4:
                    masks_img = masks_img.squeeze(1)
                m0 = normalize_mask(masks_img[0])
                video_predictor.add_new_mask(
                    inference_state=inference_state,
                    frame_idx=int(init_idx),
                    obj_id=1,
                    mask=m0
                )
                prev_segment_mask = m0
                prev_stable_mask = m0
                lost_cnt = 0
                found_cnt = FOUND_PATIENCE
            else:
                print("[warn] No detections found in initial search for this segment.")
                prev_segment_mask = None
                prev_stable_mask = None
                lost_cnt = LOST_PATIENCE  # è¦–ç‚ºéºå¤±ç‹€æ…‹é–‹å§‹
                found_cnt = 0
        else:
            # åªæœ‰åœ¨ã€Œééºå¤±ç‹€æ…‹ã€æ‰ carry mask
            video_predictor.add_new_mask(
                inference_state=inference_state,
                frame_idx=0, obj_id=1,
                mask=normalize_mask(prev_segment_mask)
            )

        # æ¨é€²
        video_segments = {}
        for fidx, obj_ids, mask_logits in video_predictor.propagate_in_video(inference_state):
            segs = {}
            global_fidx = start + int(fidx)

            # ç•¶å‰é è¨­ç‚ºã€Œç„¡ç‰©ä»¶ã€ï¼Œåƒ…åœ¨é€šéæª¢æ ¸æ™‚æ‰äº¤ä»˜ segs[1]
            current_mask_bin = None
            current_is_confident = False

            for i, obj_id in enumerate(obj_ids):
                if obj_id != 1:
                    continue
                logits_np = mask_logits[i].detach().cpu().numpy()
                _, m_bin, p_inside, area_frac = logits_to_conf_and_bin(logits_np)
                # ä¿¡å¿ƒï¼‹é¢ç©é›™é–¥é–€
                current_is_confident = (p_inside >= MASK_CONF_MIN) and (AREA_FRAC_MIN <= area_frac <= AREA_FRAC_MAX)
                if current_is_confident:
                    current_mask_bin = normalize_mask(m_bin)
                break  # å–®ä¸€ç‰©ä»¶

            # ä½ä¿¡å¿ƒå°±å…ˆç•¶ä½œã€Œç„¡ã€ï¼›é«˜ä¿¡å¿ƒç´¯è¨ˆ found_cnt
            if current_is_confident:
                found_cnt += 1
                lost_cnt = 0
                if found_cnt >= FOUND_PATIENCE:
                    segs[1] = current_mask_bin
                    prev_stable_mask = current_mask_bin
                    prev_segment_mask = current_mask_bin
            else:
                found_cnt = 0
                lost_cnt += 1
                # ä¸ç›´æ¥è£œæ´ï¼Œå…ˆè¦–ç‚ºç„¡ç‰©ä»¶ï¼›æ ¹æ“šç­–ç•¥åˆ¤æ–·æ˜¯å¦è¦ recheck

            # é€±æœŸæ€§æˆ–ä½ä¿¡å¿ƒæ‰ re-detect
            need_recheck = (not current_is_confident) or (global_fidx % RECHECK_EVERY == 0)
            if need_recheck:
                pil_img = Image.open(frame_paths[int(fidx)]).convert("RGB")
                best_m = None
                best_score = -1.0
                for (bth, tth) in THRESHOLDS:
                    boxes, scores = try_gdino_on_image(pil_img, GDINO_TEXT, bth, tth)
                    if boxes.shape[0] == 0:
                        continue
                    j = int(np.argmax(scores))
                    if scores[j] < DET_CONF_MIN:
                        continue
                    image_predictor.set_image(np.array(pil_img))
                    mimg, _, _ = image_predictor.predict(
                        point_coords=None, point_labels=None,
                        box=boxes[j:j+1], multimask_output=False
                    )
                    if mimg is not None and mimg.ndim == 4:
                        mimg = mimg.squeeze(1)
                    cand = normalize_mask(mimg[0])
                    # åˆç†é¢ç©æª¢æ ¸
                    area2 = cand.mean()
                    if not (AREA_FRAC_MIN <= area2 <= AREA_FRAC_MAX):
                        continue
                    # èˆ‡æ­·å²ç©©å®š mask åš IoU æª¢æ ¸ï¼ˆè‹¥æ²’æœ‰ç©©å®š maskï¼Œå°±ç•¥é IoUï¼‰
                    iou_ok = True if prev_stable_mask is None else (mask_iou(prev_stable_mask, cand) >= IOU_MIN)
                    if iou_ok and scores[j] > best_score:
                        best_m = cand
                        best_score = float(scores[j])

                if best_m is not None:
                    segs[1] = best_m
                    prev_stable_mask = best_m
                    prev_segment_mask = best_m
                    lost_cnt = 0
                    found_cnt = FOUND_PATIENCE  # ç›´æ¥è¦–ç‚ºç©©å®š

            # è‹¥é€£çºŒ LOST_PATIENCE å¹€ä½ä¿¡å¿ƒâ†’å®£å‘Šéºå¤±ï¼Œæ¸…æ‰ carry
            if lost_cnt >= LOST_PATIENCE:
                prev_segment_mask = None   # ä¸å† carry
                prev_stable_mask = None

            video_segments[int(fidx)] = segs

        if device == "cuda":
            torch.cuda.empty_cache()

        # ä¸‹ä¸€æ®µåˆå§‹åŒ–ï¼šåªæœ‰åœ¨ã€Œééºå¤±ä¸”æœ‰ç©©å®š maskã€æ‰æ¥åŠ›
        if len(video_segments) > 0:
            last_fidx = max(video_segments.keys())
            last_segs = video_segments[last_fidx]
            if 1 in last_segs:
                prev_segment_mask = normalize_mask(last_segs[1])
                prev_stable_mask = prev_segment_mask
                lost_cnt = 0
                found_cnt = FOUND_PATIENCE
            else:
                prev_segment_mask = None

        # ===============================================================
        # å¯«å‡º YOLOv11-seg datasetï¼ˆå–®é¡ eggï¼‰
        # ===============================================================
        for fidx, segs in sorted(video_segments.items()):
            img_np = cv2.imread(str(frame_paths[fidx]))
            if img_np is None:
                continue

            global_fidx = start + int(fidx)
            split = choose_split(global_fidx)
            if split == "train":
                IMG_DIR, LBL_DIR = IMG_DIR_TRAIN, LBL_DIR_TRAIN
            elif split == "val":
                IMG_DIR, LBL_DIR = IMG_DIR_VAL, LBL_DIR_VAL
            else:
                IMG_DIR, LBL_DIR = IMG_DIR_TEST, LBL_DIR_TEST

            base = f"frame_{global_fidx:06d}"
            img_out_path = IMG_DIR / f"{base}.jpg"
            lbl_out_path = LBL_DIR / f"{base}.txt"

            # åŸå§‹å¹€å­˜å…¥ imagesï¼ˆä¸ç–Šå¯è¦–åŒ–ï¼‰
            cv2.imwrite(str(img_out_path), all_frames[global_fidx])

            # labelï¼šè‹¥æ­¤å¹€é€šéæª¢æ ¸æ‰è¼¸å‡º polygonï¼›å¦å‰‡ç•™ç©ºæª”ï¼ˆæˆ–ä¸å¯«æª”äº¦å¯ï¼‰
            lines = []
            if 1 in segs:
                poly = mask_to_normalized_polygon(segs[1])
                if poly:
                    line = " ".join([str(TARGET_CLASS_ID)] + [f"{v:.6f}" for v in poly])
                    lines.append(line)
            with open(lbl_out_path, "w", encoding="utf-8") as f:
                f.write("\n".join(lines))

            # å¯è¦–åŒ–ï¼ˆé¸é…ï¼‰
            if 1 in segs:
                masks_np = np.stack([normalize_mask(segs[1])], axis=0).astype(bool)
                dets = sv.Detections(
                    xyxy=sv.mask_to_xyxy(masks_np),
                    mask=masks_np,
                    class_id=np.array([1], dtype=np.int32)
                )
                mask_ann = sv.MaskAnnotator(opacity=0.45)
                out = mask_ann.annotate(scene=img_np.copy(), detections=dets)
                box_ann = sv.BoxAnnotator()
                out = box_ann.annotate(out, dets)
                lbl_ann = sv.LabelAnnotator()
                out = lbl_ann.annotate(out, dets, labels=[TARGET_YOLO_NAME])
                cv2.imwrite(str(OUTPUT_DIR / f"frame_{frame_idx_global:05d}.jpg"), out)
            else:
                cv2.imwrite(str(OUTPUT_DIR / f"frame_{frame_idx_global:05d}.jpg"), img_np)

            frame_idx_global += 1

        print(f"Segment {seg_id+1} done, exported {len(video_segments)} label frames.")
        for f in tmp_dir.glob("*.jpg"):
            f.unlink()

# ===============================================================
# çµå°¾ï¼šåˆä½µå½±ç‰‡ + dataset.yaml
# ===============================================================
print("\nCombining all annotated frames into final video...")
create_video_from_images(OUTPUT_DIR, OUTPUT_VIDEO_PATH)
print(f"âœ… Done! Video saved to {OUTPUT_VIDEO_PATH}")

DATASET_YAML.write_text(
    "path: dataset\n"
    "train: images/train\n"
    "val: images/val\n"
    "test: images/test\n"
    "names:\n"
    f"  0: {TARGET_YOLO_NAME}\n",
    encoding="utf-8"
)

print("âœ… YOLOv11-seg dataset exported to:", DATASET_ROOT.resolve())
print("ğŸ‘‰ è¨“ç·´ç¤ºä¾‹ï¼šyolo task=segment mode=train model=yolo11n-seg.pt data=dataset_egg_robust/dataset.yaml imgsz=640 epochs=50 batch=16")
